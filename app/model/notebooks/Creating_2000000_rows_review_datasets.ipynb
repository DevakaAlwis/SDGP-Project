{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c96bfd91",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f19a199f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import itertools\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "650e93ba",
   "metadata": {},
   "source": [
    "# Declare filename, chunk size, range, file iteration number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1c0c01da",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"Electronics.json\"\n",
    "chunk_size = 100000\n",
    "max_objects = 2100000\n",
    "objects_read = 0\n",
    "i=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6dccb41",
   "metadata": {},
   "source": [
    "# Read 100k records and data clean and save in a CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "386b84cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 1 chunk of 100000 JSON objects\n",
      "Processed 2 chunk of 100000 JSON objects\n",
      "Processed 3 chunk of 100000 JSON objects\n",
      "Processed 4 chunk of 100000 JSON objects\n",
      "Processed 5 chunk of 100000 JSON objects\n",
      "Processed 6 chunk of 100000 JSON objects\n",
      "Processed 7 chunk of 100000 JSON objects\n",
      "Processed 8 chunk of 100000 JSON objects\n",
      "Processed 9 chunk of 100000 JSON objects\n",
      "Processed 10 chunk of 100000 JSON objects\n",
      "Processed 11 chunk of 100000 JSON objects\n",
      "Processed 12 chunk of 100000 JSON objects\n",
      "Processed 13 chunk of 100000 JSON objects\n",
      "Processed 14 chunk of 100000 JSON objects\n",
      "Processed 15 chunk of 100000 JSON objects\n",
      "Processed 16 chunk of 100000 JSON objects\n",
      "Processed 17 chunk of 100000 JSON objects\n",
      "Processed 18 chunk of 100000 JSON objects\n",
      "Processed 19 chunk of 100000 JSON objects\n",
      "Processed 20 chunk of 100000 JSON objects\n",
      "Processed 21 chunk of 100000 JSON objects\n"
     ]
    }
   ],
   "source": [
    "with open(filename) as f:\n",
    "    while objects_read < max_objects:\n",
    "        #itertools to limit the number of JSON objects read in to chunk_size\n",
    "        chunk_objects = itertools.islice(f, chunk_size)\n",
    "\n",
    "        #exit loop when there are no more JSON objects to read in, or we have read in max_objects\n",
    "        if not chunk_objects:\n",
    "            break\n",
    "\n",
    "        #json.loads to parse each JSON object in the chunk and store in a list\n",
    "        data = [json.loads(obj) for obj in chunk_objects]\n",
    "        \n",
    "        #save in a dataframe\n",
    "        df = pd.DataFrame(data)\n",
    "        \n",
    "        #create a new dataframe \n",
    "        new_review_df = pd.DataFrame()\n",
    "        \n",
    "        #assign columns to the dataframe\n",
    "        new_review_df['asin']=df['asin']\n",
    "        #if the overall value can't convert to numeric make it NAN\n",
    "        new_review_df['overall']=pd.to_numeric(df['overall'],errors='coerce') \n",
    "        new_review_df['reviewText']=df['reviewText']\n",
    "        \n",
    "        #data cleaning \n",
    "        #replace empty strings NAN values\n",
    "        new_review_df['asin'].replace('', np.nan, inplace=True)\n",
    "        new_review_df['overall'].replace('', np.nan, inplace=True)\n",
    "        new_review_df['reviewText'].replace('', np.nan, inplace=True)\n",
    "        \n",
    "        #remove NAN values rows\n",
    "        new_review_df.dropna(subset=['overall'], inplace=True)\n",
    "        new_review_df.dropna(subset=['overall'], inplace=True)\n",
    "        new_review_df.dropna(subset=['reviewText'], inplace=True)\n",
    "         \n",
    "        # drop rows that contain the partial string\n",
    "        new_review_df=new_review_df[~new_review_df.reviewText.str.contains(\"<|&nbsp;\")]\n",
    "        \n",
    "        \n",
    "        #save the dataframe to a CSV file\n",
    "        new_review_df.to_csv(\"product_reviews_for_review_analysis_\"+str(i)+\".csv\",index=False)\n",
    "                \n",
    "        print(\"Processed\", i, \"chunk of\", chunk_size, \"JSON objects\")\n",
    "        #increase the i\n",
    "        i=i+1\n",
    "        \n",
    "        # increment the objects_read counter\n",
    "        objects_read += len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72f5f477",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
